{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thin plate Spline Spatial Transformer Network\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.8.1+cu111\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from pathlib import PurePath\n",
    "import itertools\n",
    "# select GPU on the server\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "# pytorch related package \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "print('pytorch version: ' + torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# math and showcase\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (<ipython-input-2-7e89236a6b27>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7e89236a6b27>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    self.seed = 07503311\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "# stub for argparse\n",
    "class opts:\n",
    "    def __init__(self):\n",
    "        self.resume = False\n",
    "        self.sess = 'default'\n",
    "        self.seed = 7503311\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 20\n",
    "        self.n_grid_density = 5\n",
    "        self.image_size = 28\n",
    "        self.data_directory = './mnist_dis'\n",
    "        self.output_directory = './mnist_stn'\n",
    "\n",
    "args = opts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "follower: neural network of the original task. For now a simple small convolution neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class follower(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(follower, self).__init__()\n",
    "#         self.model = models.resnet18(num_classes=10)\n",
    "#         self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.model(x)\n",
    "#         return x\n",
    "\n",
    "class follower(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(follower, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "localizer, part of the STN, train on data and output the distortion kernel for TPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class localizer(nn.Module):\n",
    "    def __init__(self, args, source_points):\n",
    "        super(localizer, self).__init__()\n",
    "        self.args = args\n",
    "        n_output = args.n_grid_density**2 *2\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, n_output)\n",
    "        # insert control point as bias\n",
    "        bias = torch.flatten(source_points)\n",
    "        self.fc2.bias.data.copy_(bias)\n",
    "        self.fc2.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        # x = torch.tanh(x) # bounded\n",
    "        return x.view(self.args.batch_size, -1, 2)\n",
    "\n",
    "class affine_localizer(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(affine_localizer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 6)\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc2.bias.data.copy_(torch.tensor([0.5, 0, 0, 0, 0.5, 0], dtype=torch.float).to(device))\n",
    "        self.fc2.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tps_warper, a function that warp the image by distortion kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tps_warper(nn.Module):\n",
    "    def __init__(self, args, target_control_points):\n",
    "        super(tps_warper, self).__init__()\n",
    "        self.args = args\n",
    "        self.target_control_points = target_control_points\n",
    "        N = target_control_points.size(0)\n",
    "        # create padded kernel matrix\n",
    "        forward_kernel = torch.zeros(N + 3, N + 3)\n",
    "        target_control_partial_repr = self.compute_partial_repr(target_control_points, target_control_points)\n",
    "        forward_kernel[:N, :N].copy_(target_control_partial_repr)\n",
    "        forward_kernel[:N, -3].fill_(1)\n",
    "        forward_kernel[-3, :N].fill_(1)\n",
    "        forward_kernel[:N, -2:].copy_(target_control_points)\n",
    "        forward_kernel[-2:, :N].copy_(target_control_points.transpose(0, 1))\n",
    "        # compute inverse matrix\n",
    "        inverse_kernel = torch.inverse(forward_kernel)\n",
    "        # create target cordinate matrix\n",
    "        HW = self.args.image_size * self.args.image_size\n",
    "        target_coordinate = list(itertools.product(range(self.args.image_size), range(self.args.image_size)))\n",
    "        target_coordinate = torch.Tensor(target_coordinate).to(device) # HW x 2\n",
    "        Y, X = target_coordinate.split(1, dim = 1)\n",
    "        Y = Y * 2 / (self.args.image_size - 1) - 1\n",
    "        X = X * 2 / (self.args.image_size - 1) - 1\n",
    "        target_coordinate = torch.cat([X, Y], dim = 1) # convert from (y, x) to (x, y)\n",
    "        target_coordinate_partial_repr = self.compute_partial_repr(target_coordinate, target_control_points)\n",
    "        target_coordinate_repr = torch.cat([\n",
    "            target_coordinate_partial_repr, torch.ones(HW, 1).to(device), target_coordinate\n",
    "        ], dim = 1)\n",
    "        # register precomputed matrices\n",
    "        self.register_buffer('inverse_kernel', inverse_kernel)\n",
    "        self.register_buffer('padding_matrix', torch.zeros(3, 2).expand(self.args.batch_size, 3, 2))\n",
    "        self.register_buffer('target_coordinate_repr', target_coordinate_repr)\n",
    "\n",
    "    def compute_partial_repr(self, input_points, control_points):\n",
    "        N = input_points.size(0)\n",
    "        M = control_points.size(0)\n",
    "        pairwise_diff = input_points.view(N, 1, 2) - control_points.view(1, M, 2)\n",
    "        # original implementation, very slow\n",
    "        # pairwise_dist = torch.sum(pairwise_diff ** 2, dim = 2) # square of distance\n",
    "        pairwise_diff_square = pairwise_diff * pairwise_diff\n",
    "        pairwise_dist = pairwise_diff_square[:, :, 0] + pairwise_diff_square[:, :, 1]\n",
    "        repr_matrix = 0.5 * pairwise_dist * torch.log(pairwise_dist)\n",
    "        # fix numerical error for 0 * log(0), substitute all nan with 0\n",
    "        mask = repr_matrix != repr_matrix\n",
    "        repr_matrix.masked_fill_(mask, 0)\n",
    "        return repr_matrix\n",
    "    \n",
    "    def forward(self, image, source_control_points):\n",
    "        batch_size = source_control_points.size(0)\n",
    "        Y = torch.cat([source_control_points, self.padding_matrix], 1)\n",
    "        mapping_matrix = torch.matmul(self.inverse_kernel, Y)\n",
    "        source_coordinate = torch.matmul(self.target_coordinate_repr, mapping_matrix)\n",
    "        grid = source_coordinate.view(batch_size, self.args.image_size, self.args.image_size, 2)\n",
    "        warped_image = F.grid_sample(image, grid, align_corners=True)\n",
    "        return warped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class tps_warper(nn.Module):\n",
    "    def __init__(self, args, source_points):\n",
    "        super(tps_warper, self).__init__()\n",
    "        self.args = args\n",
    "        self.src_points = source_points\n",
    "        \n",
    "    def get_tensortype(self, tensor):\n",
    "        return dict(dtype=tensor.dtype, device=tensor.device)\n",
    "    \n",
    "    def pair_square_euclidean(self, x1, x2):\n",
    "        # ||x1 - x2||^2 = (x1-x2)^T(x1-x2) = x1^T*x1 + x2^T*x2 - 2*x1^T*x2\n",
    "        x1_sq = x1.mul(x1).sum(dim=-1, keepdim=True)\n",
    "        x2_sq = x2.mul(x2).sum(dim=-1, keepdim=True).transpose(1,2)\n",
    "        x1_x2 = x1.matmul(x2.transpose(1,2))\n",
    "        square_dist = -2 * x1_x2 + x1_sq + x2_sq\n",
    "        square_dist = square_dist.clamp(min=0)  # handle possible numerical errors\n",
    "        return square_dist\n",
    "    def kernel_distance(self, r_sq, eps=1e-8):\n",
    "        # Compute the TPS kernel distance function: r^2*log(r), where r is the euclidean distance\n",
    "        return 0.5 * r_sq * r_sq.add(eps).log()\n",
    "    \n",
    "    def get_tps_parameters(self, source_points, dest_points):\n",
    "        tensortype = self.get_tensortype(source_points)\n",
    "        batch_size, n_points = source_points.shape[:2]\n",
    "        # TPS warping has a close form equation:\n",
    "        # find a_1, a_x, a_y, w_i in some equation (a surface for the warping)\n",
    "        # to get coefficients (a_1, a_x, a_y, w_i), we need to solve a linear system\n",
    "        # build matrix L\n",
    "        pair_distance = self.pair_square_euclidean(source_points, dest_points)\n",
    "        k_matrix = self.kernel_distance(pair_distance)\n",
    "        dest_with_zeros = torch.cat((dest_points, torch.zeros(batch_size, 3, 2, **tensortype)), 1)\n",
    "        p_matrix = torch.cat((torch.ones(batch_size, n_points, 1, **tensortype), source_points), -1)\n",
    "        p_matrix_t = torch.cat((p_matrix, torch.zeros(batch_size, 3, 3, **tensortype)), 1).transpose(1,2)\n",
    "        l_matrix = torch.cat((k_matrix, p_matrix), -1)\n",
    "        l_matrix = torch.cat((l_matrix, p_matrix_t), 1)\n",
    "        # solve the linear system\n",
    "        weights, _ = torch.solve(dest_with_zeros, l_matrix)\n",
    "        rbf_weights = weights[:, :-3]\n",
    "        affine_weights = weights[:, -3:]\n",
    "        # with these weights, we got a function for TPS warping\n",
    "        return rbf_weights, affine_weights \n",
    "    \n",
    "    def tps_warp_points(self, source_points, kernel_points, rbf_weights, affine_weights):\n",
    "        # map all pixel to its new location\n",
    "        pair_distance = self.pair_square_euclidean(source_points, kernel_points)\n",
    "        k_matrix = self.kernel_distance(pair_distance)\n",
    "        # rerpeat on x and y\n",
    "        k_matrix = k_matrix.unsqueeze(3).repeat(1,1,1,2)\n",
    "        source_points = source_points.unsqueeze(3).repeat(1,1,1,2)\n",
    "        # add pixel dimension for broadcasting\n",
    "        rbf_weights, affine_weights = rbf_weights.unsqueeze(1), affine_weights.unsqueeze(1)\n",
    "\n",
    "        warped = k_matrix.mul(rbf_weights).sum(-2) + \\\n",
    "                 source_points.mul(affine_weights[:,:,1:,:]).sum(-2) + \\\n",
    "                 affine_weights[:,:,0,:]\n",
    "        return warped\n",
    "    \n",
    "    def tps_warp_image(self, image, kernel_points, rbf_weights, affine_weights):\n",
    "        tensortype = self.get_tensortype(kernel_points)\n",
    "        batch_size, _, h, w = image.shape\n",
    "        # create the grid to represent all the pixel\n",
    "        ys, xs = torch.meshgrid(torch.linspace(-1, 1, h, **tensortype),\n",
    "                                torch.linspace(-1, 1, w, **tensortype))\n",
    "        coords = torch.stack([xs, ys], -1).view(-1, 2)\n",
    "        coords = torch.stack([coords]*batch_size, 0)\n",
    "        warped = self.tps_warp_points(coords, kernel_points, rbf_weights, affine_weights).view(-1, h, w, 2)\n",
    "        warped_image = F.grid_sample(image, warped, align_corners=False)\n",
    "        return warped_image\n",
    "\n",
    "    def forward(self, image, dst_points):\n",
    "        rbf_weights, affine_weights = self.get_tps_parameters(dst_points, self.src_points)\n",
    "        warped_image = self.tps_warp_image(image, self.src_points, rbf_weights, affine_weights)\n",
    "        return warped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class affine_warper(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(affine_warper, self).__init__()\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(self, image, theta):\n",
    "        grid = F.affine_grid(theta, image.size(), align_corners=False)\n",
    "        warped_image = F.grid_sample(image, grid, align_corners=False)\n",
    "        return warped_image\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tps_stn, the structure of the whole tps-stn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tps_stn(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(tps_stn, self).__init__()\n",
    "        self.args = args\n",
    "        src_points = self.get_src_points(args.batch_size, args.n_grid_density).to(device)\n",
    "        self.src_points = src_points.unsqueeze(0).repeat(args.batch_size, 1, 1)\n",
    "\n",
    "        self.follower = follower(args)\n",
    "        self.localizer = localizer(args, src_points)\n",
    "        self.affine_localizer = affine_localizer(args)\n",
    "        self.tps_warper = tps_warper(args, src_points)\n",
    "        self.affine_warper = affine_warper(args)\n",
    "        # self.tps_warper = tps_warper(args, self.src_points)\n",
    "\n",
    "    # src_points create upon the n_grid_density is given\n",
    "    def get_src_points(self, batch_size, n_grid_density=4, grid_span=0.9):\n",
    "        src_points_1d = torch.linspace(-grid_span, grid_span, steps=n_grid_density)\n",
    "        src_points_2d = torch.cartesian_prod(src_points_1d, src_points_1d)\n",
    "        return src_points_2d\n",
    "        \n",
    "    def stn(self, x):\n",
    "        kernel_points = self.localizer(x)\n",
    "        theta = self.affine_localizer(x).view(-1, 2, 3)\n",
    "        warped_x = self.tps_warper(x, kernel_points)\n",
    "        warped_x = self.affine_warper(warped_x, theta)\n",
    "        return warped_x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.stn(x)\n",
    "        x = self.follower(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warrning: filename actually include the last dirname with it\n",
    "class ImageFolderWithFilename(datasets.ImageFolder):\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super(ImageFolderWithFilename, self).__getitem__(index)\n",
    "        path, _ = self.imgs[index]\n",
    "        path_split = PurePath(path).parts\n",
    "        filename = path_split[-2] + '/' +path_split[-1]\n",
    "        new_tuple = (original_tuple + (filename,))\n",
    "        return new_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = opts() # TODO: replace it by argparse\n",
    "    # Set seeds\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    print('==> Preparing dataset..')\n",
    "    # Training dataset\n",
    "    mean, std = (0.1307,), (0.3081,)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomAffine(\n",
    "            # degrees=15, # degree\n",
    "            # shear=(-0.15, 0.15, -0.15, 0.15),\n",
    "        # ),\n",
    "        transforms.Normalize(mean, std), \n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    train_dataset = ImageFolderWithFilename(root=args.data_directory+'/training', \\\n",
    "        transform=train_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\\\n",
    "        shuffle=True, drop_last=True, num_workers=8, pin_memory=True)\n",
    "    test_dataset = ImageFolderWithFilename(root=args.data_directory+'/testing', \\\n",
    "        transform=test_transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\\\n",
    "        shuffle=True, drop_last=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # Load model\n",
    "    if args.resume:\n",
    "        # Load checkpoint.\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        model = tps_stn(args).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        checkpoint = torch.load('./checkpoint/' + args.sess + '_' + str(args.seed) + '.pth')\n",
    "        prev_acc = checkpoint['acc']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch_start = checkpoint['epoch'] + 1\n",
    "        torch.set_rng_state(checkpoint['rng_state'])\n",
    "    else:\n",
    "        print('==> Building model..')\n",
    "        epoch_start = 0\n",
    "        prev_acc = 0.0\n",
    "        model = tps_stn(args).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Logger\n",
    "    result_folder = './results/'\n",
    "    if not os.path.exists(result_folder):\n",
    "        os.makedirs(result_folder)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logname = model.__class__.__name__ + '_' + args.sess + \\\n",
    "        '_' + str(args.seed) + '.log'\n",
    "    logfile = os.path.join(result_folder, logname)\n",
    "    if os.path.exists(logfile):\n",
    "        os.remove(logfile)\n",
    "    logging.basicConfig(\n",
    "        format='[%(asctime)s] - %(message)s',\n",
    "        datefmt='%Y/%m/%d %H:%M:%S',\n",
    "        level=logging.INFO,\n",
    "        filename=logfile\n",
    "    )\n",
    "    logger.info(args)\n",
    "\n",
    "    # Training\n",
    "    def train(epoch):\n",
    "        print('\\nEpoch: {:04}'.format(epoch))\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        model.train()\n",
    "        for batch_idx, (data, target, _) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output_logit = model(data)\n",
    "            loss = F.cross_entropy(output_logit, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = F.softmax(output_logit, dim=1)\n",
    "            preds_top_p, preds_top_class = preds.topk(1, dim=1)\n",
    "\n",
    "            train_loss += loss.item() * target.size(0)\n",
    "            total += target.size(0)\n",
    "            correct += (preds_top_class.view(target.shape) == target).sum().item()\n",
    "\n",
    "        return (train_loss / batch_idx, 100. * correct / total)\n",
    "    # Test\n",
    "    def test(epoch):\n",
    "        model.eval()\n",
    "        # load the best state_dict\n",
    "        checkpoint = torch.load('./checkpoint/' + args.sess + '_' + str(args.seed) + '.pth')\n",
    "        prev_acc = checkpoint['acc']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        torch.set_rng_state(checkpoint['rng_state'])\n",
    "        test_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target, _) in enumerate(test_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                output_logit = model(data)\n",
    "                loss = F.cross_entropy(output_logit, target)\n",
    "                preds = F.softmax(output_logit, dim=1)\n",
    "                preds_top_p, preds_top_class = preds.topk(1, dim=1)\n",
    "    \n",
    "                test_loss += loss.item() * target.size(0)\n",
    "                total += target.size(0)\n",
    "                correct += (preds_top_class.view(target.shape) == target).sum().item()\n",
    "        \n",
    "        return (test_loss / batch_idx, 100. * correct / total)\n",
    "    # Adjust testset using STN\n",
    "    def convert():\n",
    "        model.eval()\n",
    "        # load the best state_dict\n",
    "        checkpoint = torch.load('./checkpoint/' + args.sess + '_' + str(args.seed) + '.pth')\n",
    "        prev_acc = checkpoint['acc']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        torch.set_rng_state(checkpoint['rng_state'])\n",
    "        # create folders\n",
    "        testing_directory = args.output_directory + '/testing/'\n",
    "        if not os.path.exists(testing_directory):\n",
    "            os.makedirs(testing_directory)\n",
    "        for classes in range(10):\n",
    "            if not os.path.exists(testing_directory + str(classes)):\n",
    "                os.makedirs(testing_directory + str(classes))\n",
    "        print('==> Converting testing dataset using STN..')\n",
    "        mean, std = (0.1307,), (0.3081,)\n",
    "        inv_normalize = transforms.Normalize(\n",
    "            mean= [-m/s for m, s in zip(mean, std)],\n",
    "            std= [1/s for s in std]\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target, filename) in enumerate(test_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output_image = model.stn(data)\n",
    "                output_image = inv_normalize(output_image)\n",
    "                output_image = torch.clamp(output_image, 0, 1)\n",
    "                \n",
    "                for index, f in enumerate(filename):\n",
    "                    image = output_image[index].detach().cpu()\n",
    "                    f = testing_directory + f # it includes last dirname (same as target)\n",
    "                    torchvision.utils.save_image(image, f)\n",
    "    # visualize\n",
    "    def visualize():\n",
    "        model.eval()\n",
    "        # load the best state_dict\n",
    "        checkpoint = torch.load('./checkpoint/' + args.sess + '_' + str(args.seed) + '.pth')\n",
    "        prev_acc = checkpoint['acc']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        torch.set_rng_state(checkpoint['rng_state'])\n",
    "        mean, std = (0.1307,), (0.3081,)\n",
    "        inv_normalize = transforms.Normalize(\n",
    "            mean= [-m/s for m, s in zip(mean, std)],\n",
    "            std= [1/s for s in std]\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            data = next(iter(test_loader))[0].to(device)\n",
    "            input_image = inv_normalize(data)\n",
    "            input_image = torch.clamp(input_image, 0, 1)\n",
    "            output_image = model.stn(data)\n",
    "            output_image = inv_normalize(output_image)\n",
    "            output_image = torch.clamp(output_image, 0, 1)\n",
    "            in_grid = torchvision.utils.make_grid(input_image.detach().cpu()).permute(1,2,0).numpy()\n",
    "            out_grid = torchvision.utils.make_grid(output_image.detach().cpu()).permute(1,2,0).numpy()\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(24, 12))\n",
    "            axs[0].imshow(in_grid)\n",
    "            axs[0].set_title('Dataset Images')\n",
    "            axs[1].imshow(out_grid)\n",
    "            axs[1].set_title('Transformed Images')\n",
    "            plt.show()\n",
    "    # Save checkpoint\n",
    "    def checkpoint(acc, epoch):\n",
    "        print('==> Saving..')\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        save_path = './checkpoint/' + args.sess + '_' + str(args.seed) + '.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'acc': acc,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'rng_state': torch.get_rng_state(),\n",
    "            }, save_path)\n",
    "    \n",
    "    # Run\n",
    "    logger.info('Epoch \\t Seconds \\t \\t Train Loss \\t Train Acc')\n",
    "    start_train_time = time.time()\n",
    "    for epoch in range(epoch_start, args.epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(epoch)\n",
    "        epoch_time = time.time()\n",
    "        logger.info('%5d \\t %7.1f \\t \\t %10.4f \\t %9.4f',\n",
    "            epoch, epoch_time - start_epoch_time, train_loss, train_acc)\n",
    "        # Save checkpoint.\n",
    "        if train_acc - prev_acc  > 0.1:\n",
    "            prev_acc = train_acc\n",
    "            checkpoint(train_acc, epoch)\n",
    "    train_time = time.time()\n",
    "    logger.info('Total train time: %.4f minutes', (train_time - start_train_time)/60)\n",
    "\n",
    "    # Evaluation\n",
    "    logger.info('Test Loss \\t Test Acc')\n",
    "    test_loss, test_acc = test(epoch)\n",
    "    logger.info('%9.4f \\t %8.4f', test_loss, test_acc)\n",
    "\n",
    "    # Converting testing dataset using STN..\n",
    "    convert()\n",
    "    print('==> Done.')\n",
    "    \n",
    "    visualize()\n",
    "\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a45e29939697a8e34511f302731d6e6b66b2c8f4dd6bd0c90c9d1188a2e1a91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9.5': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}